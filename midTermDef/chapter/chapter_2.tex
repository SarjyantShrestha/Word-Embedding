\chapter{Literature Review}
\noindent
Most of the research that has been undertaken on the Nepali corpus was focusing on generating embeddings through traditional approaches like TFIDF, Wod2Vec and other embedding methods. \cite{Bhatta_Shrestha_Nepal_Pandey_Koirala_2020} \cite{Singh2019NepaliMT} \cite{shahi2018nepali} and \cite{ghosh2018class} implemented TF-IDF on Nepali text for text classification and other purposes such as sentiment analysis. Similarly, Word2Vec approach in nepali corpus was implemented by \cite{Bhatta_Shrestha_Nepal_Pandey_Koirala_2020} \cite{kaushal2016} and \cite{basnet2018improving}. 300-Dimensional Word Embeddings for Nepali Language \cite{300D} has pre-trained Word2Vec model having 300-dimensional vectors for more than 0.5 million Nepali words and phrases. The embeddings generated using the methods described above are static, implying that each word retains only one vector representation regardless of its context of use. However, contemporary trends emphasize the adoption of contextual-dependent embeddings over their contextual-independent counterparts. As highlighted earlier, there have been limited studies on BERT within the Nepali context. \cite{koirala-niraula-2021-npvec1} claimed to provide an efficient Nepali BERT embedding, but despite having a huge dataset they were short of computational resources due to which they had to compromise on the different BERT parameters. They modified the BERT model by averaging the hidden states from the last two hidden layers to get the embeddings, whereas, for getting the baseline results, instead of using any pre-trained word vectors, a trainable Keras embedding layer was used in front of the architecture mentioned above which automatically learns the word embeddings by only using the provided training examples. \cite{rajan_nepalibert_2021} and  \cite{milanmg_bert-nepali_2022} also tried the capacity of BERT for cross-lingual in Natural Language Processing. \\\\
There are also studies done in XLM \cite{NEURIPS2019_c04c19c2}. The paper compares a Nepali language model with a cross-lingual language model trained in Nepali but enriched with different combinations of Hindi and English data, showing how leveraging data from related languages can benefit low-resource languages like Nepali.
300-Dimensional Word Embeddings for Nepali Language \cite{300D} has pre-trained Word2Vec model having 300-dimensional vectors for more than 0.5 million Nepali words and phrases.
The embeddings generated using the methods described above are static, implying that each word retains only one vector representation regardless of its context of use. However, contemporary trends emphasize the adoption of contextual-dependent embeddings over their contextual-independent counterparts. As highlighted earlier, there have been limited studies on BERT within the Nepali context. \cite{koirala-niraula-2021-npvec1} claimed to provide an efficient Nepali BERT embedding, but despite having a huge dataset they were short of computational resources due to which they had to compromise on the different BERT parameters. They modified the BERT model by averaging the hidden states from the last two hidden layers to get the embeddings, whereas, for getting the baseline results, instead of using any pre-trained word vectors, a trainable Keras embedding layer was used in front of the architecture mentioned above which automatically learns the word embeddings by only using the provided training examples. \cite{rajan_nepalibert_2021} and  \cite{milanmg_bert-nepali_2022} also tried the capacity of BERT for cross-lingual in Natural Language Processing. \\\\
There are also studies done in XLM \cite{NEURIPS2019_c04c19c2}. The paper compares a Nepali language model with a cross-lingual language model trained in Nepali but enriched with different combinations of Hindi and English data, showing how leveraging data from related languages can benefit low-resource languages like Nepali.
