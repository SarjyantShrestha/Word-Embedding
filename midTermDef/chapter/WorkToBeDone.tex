\chapter{Work to be done}
\section{Expand Dataset Further}
Increase the size and diversity of the dataset by collecting more Nepali legal texts. This will improve the model's ability to generalize and perform well on a wider range of inputs.

\section{Prepare Transformer-Based Embedding Layer and Self-Attention Layers}
Implement the core components of a transformer model, including embedding layers to convert words into numerical representations and self-attention layers to capture dependencies between words in the text. These components are crucial for understanding contextual relationships within the data.

\section{Train, Test, and Validate the Model}
Train the model using the expanded dataset to learn from the data and optimize its performance on legal text tasks. Test the model on separate datasets to evaluate its accuracy and generalization capabilities. Validate the model's performance against predefined metrics to ensure it meets quality standards.

\section{Hyperparameter Tuning}
Fine-tune the model by adjusting hyperparameters such as learning rate, batch size, and dropout rates. This process aims to optimize model performance and efficiency, leading to better results during training and inference.