\chapter{Introduction}
\pagenumbering{arabic}
\section{Background Introduction}
NLP is a branch of linguitics, computer science, and artificial intelligence concerned with computer human interaction, mainly how to design computers to process and evaluate huge volumes of natural language data \cite{asudani2023impact}. Pre-training of an NLP model plays an essential role in transfer learning, where a language model will be trained on a vast corpus set and later fine-tune the model for a specific purpose \cite{NepaliBERT}. Word embedding is a fundamental concept in NLP. It is a real-valued vector representation of words by embedding both semantic and syntactic meanings obtained from unlabeled large corpus \cite{Wang_Wang_Chen_Wang_Kuo_2019}. It is of n-dimensional distributed representation of a text that attempts to capture the meanings of the words \cite{asudani2023impact}. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers \cite{enwiki:1219561882}. Pre-trained word embeddings encode general word semantics and lexical regularities of natural language, and have proven useful across many NLP tasks, including word sense disambiguation, machine translation, and sentiment analysis, to name a few \cite{moreo2019wordclass}. Word embeddings have been found to be very useful for many NLP tasks, including but not limited to Chunking \cite{turian-etal-2010-word}, Question Answering \cite{questionanswer}, Parsing and Sentiment Analysis\cite{sentimentanalysis}. \cite{wordembedding} \\\\
\textbf{Types of Word Embedding Techniques} \cite{reviewOnWordEmbedding} \\
\textbf{Traditional Embeddings}: Traditional word embeddings represent words as fixed vectors in an n-dimensional space, capturing semantic relationships between words.
These embeddings are static and do not change based on context or training data. \\
\textbf{Static Embeddings}:
Static word embeddings are pre-trained on a large corpus of text and do not change during model training.
They are useful for tasks where word meanings remain constant across different contexts.\\
\textbf{Contextualized Embeddings}:
Contextualized word embeddings, like BERT, are based on transformer models that can capture word meanings based on the context in which they appear.
These embeddings provide more accurate representations of words by considering the surrounding context during training. \\
\textbf{Combined Word Embedding and Neural Network Models}:
Combining word embeddings with neural network models can enhance model accuracy in various natural language processing tasks such as sentiment classification, text categorization, and phrase prediction. \\
Nepali is one of the languages that uses Devanagari, a script used in many languages spoken in Asian countries. It is spoken by more than 20 million people, mainly in Nepal, and other places in the world including Bhutan, India and Myanmar \cite{niraula2020linguistic}.  It has been rarely used for Natural Language Processing services.  Nepali can be quite complex due to its many sounds, grammar rules, and ways to change words. Due to its complex grammatical structure and rich characters, extracting fruitful information from the corpus has been challenging \cite{NepaliBERT}.\\
The advancement of NLP technologies adapted to individual languages, like Nepali, hold immense potential for empowering communities and enhancing accessibility to digital resources for Nepali language. By filling the gap between technological innovation and linguistic diversity, we can unlock new possibilities for communication and education.\\
\section{Problem Statement} 
Even though Word Embeddings can be directly learned from raw texts in an unsupervised fashion, gathering a large amount of data for its training remains a huge challenge in itself for a low-resource language such as Nepali \cite{koirala-niraula-2021-npvec1}. Despite having breakthroughs in the field of NLP, productive results with the Nepali language have not been achieved. One of the primary reasons for this is the need for more computational resources \cite{NepaliBERT}. As mentioned in the most recent study in this topic (i.e NepaliBERT \cite{NepaliBERT}), there is a lack of larger, more diverse, and context-rich dataset to enhance the accuracy and robustness of the word embeddings in Nepali language. This research study seeks to construct a more finely tuned model capable of generating embeddings for the Nepali corpus. It is seen that there is reduction of perplexity by using XLM.
\section{Objective}
    The main aim of this project is:
    \begin{itemize}
        \item To develop context dependent word embedding for Nepali language.
    \end{itemize}
