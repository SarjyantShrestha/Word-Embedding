{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9997777,"sourceType":"datasetVersion","datasetId":6152703}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport unicodedata\nimport string\nimport torch\nfrom datasets import Dataset\nfrom collections import Counter\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntorch.manual_seed(7)\ntorch.cuda.manual_seed(7)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kantipur = pd.read_csv('/kaggle/input/kantipur-dataset/cleaned_combined_kantipur.tsv',sep='\\t', header=None)\nkantipur.columns=['title','news']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class NepaliTokenizer:\n    def __init__(self):\n        # Nepali-specific character ranges and rules\n        self.NEPALI_DEVANAGARI_RANGE = (0x0900, 0x097F)\n        \n        # Punctuation and special characters to handle\n        self.NEPALI_PUNCTUATION = r'।॥,\\.;:!?\\(\\)\\[\\]\\{\\}'\n        \n        # Common Nepali suffixes and postpositions to potentially separate\n        self.NEPALI_SUFFIXES = [\n            'ले', 'को', 'का', 'की', 'के', \n            'मा', 'बाट', 'सँग', 'देखि', \n            'सम्म', 'पछि', 'अघि',\n            'हरू'  # Plural marker\n        ]\n    \n    def is_nepali_character(self, char):\n        \"\"\"\n        Check if a character is in the Devanagari script range used for Nepali\n        \n        Args:\n        - char: Single character to check\n        \n        Returns:\n        - Boolean indicating if character is Nepali\n        \"\"\"\n        if not char:\n            return False\n        \n        # Get the Unicode code point of the character\n        code_point = ord(char)\n        \n        # Check if it falls within Devanagari range\n        return (self.NEPALI_DEVANAGARI_RANGE[0] <= code_point <= self.NEPALI_DEVANAGARI_RANGE[1])\n    \n    def normalize_nepali_text(self, text):\n        \"\"\"\n        Normalize Nepali text\n        \n        Args:\n        - text: Input text to normalize\n        \n        Returns:\n        - Normalized text\n        \"\"\"\n        \n        # Normalize Unicode decomposition\n        text = unicodedata.normalize('NFC', text)\n        \n        # Replace multiple spaces with single space\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        return text\n    \n    def tokenize(self, text):\n        \"\"\"\n        Advanced Nepali tokenization method\n        \n        Args:\n        - text: Input text to tokenize\n        \n        Returns:\n        - List of tokens\n        \"\"\"\n        # Normalize the text first\n        text = self.normalize_nepali_text(text)\n        \n        # Tokenization strategy\n        tokens = []\n        \n        # Current token being built\n        current_token = []\n        \n        # Iterate through characters\n        for i, char in enumerate(text):\n            # Check if character is Nepali, space, or punctuation\n            if self.is_nepali_character(char):\n                current_token.append(char)\n            elif char.isspace():\n                # If we have a current token, add it\n                if current_token:\n                    tokens.append(''.join(current_token))\n                    current_token = []\n            elif char in self.NEPALI_PUNCTUATION:\n                # Add current token if exists\n                if current_token:\n                    tokens.append(''.join(current_token))\n                    current_token = []\n                \n                # Add punctuation as separate token\n                tokens.append(char)\n            else:\n                # Non-Nepali characters (like digits, Latin script)\n                if current_token:\n                    tokens.append(''.join(current_token))\n                    current_token = []\n                tokens.append(char)\n        \n        # Add last token if exists\n        if current_token:\n            tokens.append(''.join(current_token))\n        \n        # Suffix and postposition handling\n        final_tokens = []\n        for token in tokens:\n            # Check for tokens that can be further split\n            if self.is_nepali_character(token[-1]):\n                # Check for known suffixes\n                for suffix in self.NEPALI_SUFFIXES:\n                    if token.endswith(suffix):\n                        base_word = token[:-len(suffix)]\n                        if base_word:\n                            final_tokens.append(base_word)\n                            final_tokens.append(suffix)\n                            break\n                else:\n                    final_tokens.append(token)\n            else:\n                final_tokens.append(token)\n        \n        return final_tokens","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomBERTTokenizer:\n    def __init__(self, \n                 max_vocab_size=30000, \n                 max_length=512, \n                 mask_probability=0.15):\n        \"\"\"\n        Custom BERT-style tokenizer\n        \n        Args:\n        - max_vocab_size: Maximum number of tokens in vocabulary\n        - max_length: Maximum sequence length\n        - mask_probability: Probability of masking a token\n        \"\"\"\n        self.max_length = max_length\n        self.mask_probability = mask_probability\n        \n        # Special tokens\n        self.special_tokens = {\n            '[PAD]': 0,\n            '[UNK]': 1,\n            '[CLS]': 2,\n            '[SEP]': 3,\n            '[MASK]': 4\n        }\n        self.special_token_ids = {token: idx for token, idx in self.special_tokens.items()}\n        \n        # Vocabulary will be built dynamically\n        self.vocab = self.special_tokens.copy()\n        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n        \n        # Keep track of token frequencies\n        self.token_freq = Counter()\n        \n        # Tokenization parameters\n        self.max_vocab_size = max_vocab_size\n        self.tokenizer = NepaliTokenizer()\n\n    def _tokenize(self, text):\n        \"\"\"\n        Basic tokenization method\n        \n        Args:\n        - text: Input text to tokenize\n        \n        Returns:\n        - List of tokens\n        \"\"\"\n\n        tokens = self.tokenizer.tokenize(text)\n        \n        return tokens\n\n    def build_vocab(self, texts):\n        \"\"\"\n        Build vocabulary from corpus\n        \n        Args:\n        - texts: List of texts to build vocabulary from\n        \"\"\"\n        # Tokenize all texts\n        all_tokens = []\n        for text in texts:\n            tokens = self._tokenize(text)\n            all_tokens.extend(tokens)\n            self.token_freq.update(tokens)\n        \n        # Sort tokens by frequency\n        sorted_tokens = sorted(\n            self.token_freq.items(), \n            key=lambda x: x[1], \n            reverse=True\n        )\n        \n        # Add most frequent tokens to vocabulary\n        next_idx = max(self.special_token_ids.values()) + 1\n        for token, _ in sorted_tokens:\n            if token not in self.vocab:\n                self.vocab[token] = next_idx\n                self.reverse_vocab[next_idx] = token\n                next_idx += 1\n                \n                # Stop if we reach max vocab size\n                if len(self.vocab) >= self.max_vocab_size:\n                    break\n    \n    def encode(self, text):\n        \"\"\"\n        Encode text to token ids\n        \n        Args:\n        - text: Input text\n        \n        Returns:\n        - List of token ids\n        \"\"\"\n        \n        # Tokenize\n        tokens = self._tokenize(text)\n    \n        # Convert to ids, using [UNK] for out-of-vocab tokens\n        token_ids = [\n            self.vocab.get(token, self.special_token_ids['[UNK]'])\n            for token in tokens\n        ]\n    \n        # Add special tokens\n        token_ids = [self.special_token_ids['[CLS]']] + \\\n                    token_ids + \\\n                    [self.special_token_ids['[SEP]']]\n    \n        # Truncate or pad to max_length\n        token_ids = token_ids[:self.max_length]\n        token_ids += [self.special_token_ids['[PAD]']] * (self.max_length - len(token_ids))\n    \n        return token_ids\n\n    def mask_tokens(self, input_ids):\n        \"\"\"\n        Apply token masking\n        \n        Args:\n        - input_ids: Original token sequence\n        \n        Returns:\n        - masked_input_ids: Input with some tokens masked\n        - mask_labels: Original tokens before masking\n        \"\"\"\n        # Ensure input_ids is a torch tensor\n        if not isinstance(input_ids, torch.Tensor):\n            input_ids = torch.tensor(input_ids)\n        \n        # Create a copy of input_ids\n        masked_input_ids = input_ids.clone()\n        \n        # Create mask for tokens to be masked (excluding special tokens)\n        mask = torch.bernoulli(torch.full(masked_input_ids.shape, self.mask_probability)).bool()\n        mask &= (masked_input_ids != self.special_token_ids['[CLS]']) & \\\n               (masked_input_ids != self.special_token_ids['[SEP]']) & \\\n               (masked_input_ids != self.special_token_ids['[PAD]'])\n        \n        # If no tokens are masked, randomly mask at least one\n        if not mask.any():\n            # Randomly select a non-special token to mask\n            non_special_mask = (masked_input_ids != self.special_token_ids['[CLS]']) & \\\n                               (masked_input_ids != self.special_token_ids['[SEP]']) & \\\n                               (masked_input_ids != self.special_token_ids['[PAD]'])\n            if non_special_mask.any():\n                random_index = torch.multinomial(non_special_mask.float(), 1)[0]\n                mask[random_index] = True\n        \n        # Create labels for masked tokens\n        mask_labels = torch.zeros_like(masked_input_ids)\n        mask_labels[mask] = masked_input_ids[mask]\n        \n        # 80% of masked tokens are replaced with [MASK]\n        mask_mask = mask & (torch.rand_like(masked_input_ids.float()) < 0.8)\n        masked_input_ids[mask_mask] = self.special_token_ids['[MASK]']\n        \n        # 10% of masked tokens are replaced with random tokens\n        if mask.any():\n            random_tokens = torch.randint_like(\n                masked_input_ids, \n                0, \n                len(self.vocab)\n            )\n            random_mask = mask & (torch.rand_like(masked_input_ids.float()) < 0.1)\n            masked_input_ids[random_mask] = random_tokens[random_mask]\n        \n        return masked_input_ids, mask_labels\n\n    def prepare_bert_pretraining_data(self, df, text_column):\n        \"\"\"\n        Prepare BERT pretraining data from a DataFrame\n        \n        Args:\n        - df: Input DataFrame\n        - text_column: Name of the text column\n        \n        Returns:\n        - Tuple of tensors for pretraining, now with masked_tokens\n        \"\"\"\n        # First, build vocabulary\n        self.build_vocab(df[text_column])\n        \n        # Prepare lists to store data\n        input_sequences = []\n        segment_ids = []\n        masked_tokens = []\n        \n        # Iterate through the DataFrame\n        for i in range(len(df)):\n            try:\n                text1 = df[text_column].iloc[i]\n                \n                input_ids1 = self.encode(text1)\n                \n                # Combine texts with segment ids\n                combined_input_ids = input_ids1\n                segment_ids_tensor = torch.zeros(len(combined_input_ids), dtype=torch.long)\n                segment_ids_tensor[len(input_ids1):] = 1\n\n                #AFTER GPU ON\n                masked_input_ids, mask_label = self.mask_tokens(combined_input_ids)\n                # Append to lists\n                input_sequences.append(torch.tensor(combined_input_ids).clone().detach())\n                segment_ids.append(segment_ids_tensor.clone().detach())\n                masked_tokens.append(mask_label.clone().detach())\n\n            except Exception as e:\n                print(f\"Error processing row {i}: {e}\")\n                continue\n        \n        # Convert to tensors\n        input_sequences = torch.stack(input_sequences)\n        segment_ids = torch.stack(segment_ids)\n        masked_tokens = torch.stack(masked_tokens)\n        \n        return input_sequences, segment_ids, masked_tokens","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PretrainingDataset(Dataset):\n    def __init__(self, input_sequences, segment_ids, masked_tokens):\n        self.input_sequences = input_sequences\n        self.segment_ids = segment_ids\n        self.masked_tokens = masked_tokens\n    def __len__(self):\n        return len(self.input_sequences)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_sequences':torch.tensor(self.input_sequences[idx], dtype=torch.long),\n            'segment_ids':torch.tensor(self.segment_ids[idx], dtype=torch.long),\n            'masked_tokens':torch.tensor(self.masked_tokens[idx],  dtype=torch.long)\n         }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = CustomBERTTokenizer()\n\ninput_sequences, segment_ids, masked_tokens= tokenizer.prepare_bert_pretraining_data(kantipur, 'news')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(segment_ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(0.8*len(input_sequences))\ntest_size = int(0.2*len(input_sequences))\nprint(f\"Train_size= {train_size}\")\nprint(f\"Test_size= {test_size}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = PretrainingDataset(input_sequences[:train_size], segment_ids[:train_size], masked_tokens[:train_size])\ntest_dataset = PretrainingDataset(input_sequences[train_size:], segment_ids[train_size:], masked_tokens[train_size:])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(test_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True,pin_memory=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True,pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"code","source":"class BERTEmbedding(nn.Module):\n    def __init__(self, vocab_size, n_segments, max_len, embed_dim, dropout):\n        super().__init__()\n        self.tok_embed = nn.Embedding(vocab_size, embed_dim)  # Token embedding\n        self.seg_embed = nn.Embedding(n_segments, embed_dim)  # Segment embedding\n        self.pos_embed = nn.Embedding(max_len, embed_dim)     # Positional embedding\n        self.drop = nn.Dropout(dropout)\n        self.max_len = max_len  # Store max length for positional embedding\n\n    def forward(self, seq, seg):\n        # Dynamically generate position indices on the same device as `seq`\n        pos_inp = torch.arange(seq.size(1), device=seq.device).unsqueeze(0).expand_as(seq)\n        embed_val = self.tok_embed(seq) + self.seg_embed(seg) + self.pos_embed(pos_inp)\n        embed_val = self.drop(embed_val)\n        return embed_val\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERT(nn.Module):\n    def __init__(self,\n                 vocab_size,\n                 n_segments,\n                 max_len,\n                 embed_dim,\n                 n_layers,\n                 attn_heads,\n                 dropout):\n        super().__init__()\n        self.embedding = BERTEmbedding(vocab_size, n_segments, max_len, embed_dim, dropout)\n        self.encoder_layer = nn.TransformerEncoderLayer(embed_dim, attn_heads, embed_dim*4)\n        self.encoder_block = nn.TransformerEncoder(self.encoder_layer, n_layers)\n    def forward(self, seq, seg):\n        out = self.embedding(seq, seg)\n        out = self.encoder_block(out)\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERTPretrainingModel(nn.Module):\n    def __init__(self, bert_model, vocab_size):\n        super().__init__()\n        self.bert = bert_model\n        # MLM head\n        self.mlm_head = nn.Linear(\n            bert_model.embedding.tok_embed.embedding_dim, vocab_size)\n\n    def forward(self, seq, seg):\n        # Get BERT embeddings\n        bert_output = self.bert(seq, seg)  # [batch_size, seq_len, embed_dim]\n\n        # MLM prediction for all tokens\n        mlm_prediction = self.mlm_head(bert_output)  # [batch_size, seq_len, vocab_size]\n\n        return mlm_prediction","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train_bert(model, train_dataloader, optimizer, mlm_criterion, device):\n    model.train()\n    total_train_loss = 0\n\n    for batch in tqdm(train_dataloader):\n        seq = batch['input_sequences'].to(device)  # [batch_size, seq_len]\n        seg = batch['segment_ids'].to(device)      # [batch_size, seq_len]\n        masked_tokens = batch['masked_tokens'].to(device)  # [batch_size, seq_len]\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        mlm_predictions = model(seq, seg)  # [batch_size, seq_len, vocab_size]\n\n        # Flatten predictions and targets\n        mlm_predictions = mlm_predictions.view(-1, mlm_predictions.size(-1))  # [batch_size * seq_len, vocab_size]\n        masked_tokens = masked_tokens.view(-1)  # [batch_size * seq_len]\n\n        # Compute MLM loss\n        mlm_loss = mlm_criterion(mlm_predictions, masked_tokens)\n\n        # Backward pass\n        mlm_loss.backward()\n\n        # Optimizer step\n        optimizer.step()\n\n        # Accumulate loss\n        total_train_loss += mlm_loss.item()\n\n    return total_train_loss / len(train_dataloader)\n\ndef test_bert(model,test_dataloader,mlm_criterion,device):\n    model.eval()\n    total_test_loss = 0\n    for batch in tqdm(test_dataloader):\n        seq = batch['input_sequences'].to(device)  # [batch_size, seq_len]\n        seg = batch['segment_ids'].to(device)      # [batch_size, seq_len]\n        masked_tokens = batch['masked_tokens'].to(device)  # [batch_size, seq_len]\n\n        # Forward pass\n        mlm_predictions = model(seq, seg)  # [batch_size, seq_len, vocab_size]\n\n        # Flatten predictions and targets\n        mlm_predictions = mlm_predictions.view(-1, mlm_predictions.size(-1))  # [batch_size * seq_len, vocab_size]\n        masked_tokens = masked_tokens.view(-1)  # [batch_size * seq_len]\n\n        # Compute MLM loss\n        mlm_loss = mlm_criterion(mlm_predictions, masked_tokens)\n\n        # Accumulate loss\n        total_test_loss += mlm_loss.item()\n\n    return total_test_loss / len(test_dataloader)\n\n        \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\nvocab_size = 30522  # Example vocab size\nn_segments = 2\nmax_len = 512\nembed_dim = 768\nn_layers = 12\nattn_heads = 12\ndropout = 0.1\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize BERT model\nbert_base = BERT(\n    vocab_size=vocab_size, \n    n_segments=n_segments, \n    max_len=max_len, \n    embed_dim=embed_dim, \n    n_layers=n_layers, \n    attn_heads=attn_heads, \n    dropout=dropout\n)\n\n# Wrap BERT in pretraining model\nmodel = BERTPretrainingModel(bert_base, vocab_size).to(device)\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Loss functions]\nmlm_criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n\n\n# Training loop\nnum_epochs = 10\ntrain_loss = []\ntest_loss = []\nfor epoch in range(num_epochs):\n    train_mlm_loss= train_bert(\n        model, \n        train_dataloader, \n        optimizer, \n        mlm_criterion, \n        device\n    )\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    print(f'Train MLM Loss: {train_mlm_loss:.4f}')\n    train_loss.append(train_mlm_loss)\n    \n    test_mlm_loss= test_bert(\n        model, \n        test_dataloader,\n        mlm_criterion,\n        device\n    )\n    print(f'Test MLM Loss: {test_mlm_loss:.4f}')\n    test_loss.append(test_mlm_loss)\n    \n# Save the model\ntorch.save(model, 'model.pth')\ntorch.save(model.state_dict(), 'model_state_dict.pth')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('train_loss.txt', 'w') as traintxt:\n    traintxt.write(str(train_loss))\n\nwith open('test_loss.txt', 'w') as testtxt:\n    testtxt.write(str(test_loss))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_loss,label='train_loss',color='blue')\nplt.plot(test_loss,label='test_loss',color='red')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Epoch vs Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}