@article{asudani2023impact,
      title={Impact of word embedding models on text analytics in deep learning environment: a review},
      author={Asudani, Deepak S and Nagwani, Narendra K and Singh, Pradeep},
      journal={Artificial Intelligence Review},
      volume={22},
      number={1},
      pages={1--81},
      year={2023},
      month={Feb},
      doi={10.1007/s10462-023-10419-1},
      note={Epub ahead of print},
      PMID={36844886},
      PMCID={PMC9944441}
}
@article{Wang_Wang_Chen_Wang_Kuo_2019, title={Evaluating word embedding models: methods and experimental results}, volume={8}, DOI={10.1017/ATSIP.2019.12}, journal={APSIPA Transactions on Signal and Information Processing}, author={Wang, Bin and Wang, Angela and Chen, Fenxiao and Wang, Yuncheng and Kuo, C.-C. Jay}, year={2019}, pages={e19}} <div></div>

@misc{ enwiki:1219561882,
      author = "{Wikipedia contributors}",
      title = "Word embedding --- {Wikipedia}{,} The Free Encyclopedia",
      year = "2024",
      url = "https://en.wikipedia.org/w/index.php?title=Word_embedding&oldid=1219561882",
      note = "[Online; accessed 4-May-2024]"
}

@inproceedings{NepaliBERT,
author = {Pudasaini, Shushanta and Shakya, Subarna and Tamang, Aakash and Adhikari, Sajjan and Thapa, Sunil and Lamichhane, Sagar},
year = {2023},
month = {10},
pages = {325-330},
title = {NepaliBERT: Pre-training of Masked Language Model in Nepali Corpus},
doi = {10.1109/I-SMAC58438.2023.10290690}
}


@misc{moreo2019wordclass,
      title={Word-Class Embeddings for Multiclass Text Classification}, 
      author={Alejandro Moreo and Andrea Esuli and Fabrizio Sebastiani},
      year={2019},
      eprint={1911.11506},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{niraula2020linguistic,

      title={Linguistic Taboos and Euphemisms in Nepali}, 
      author={Nobal B. Niraula and Saurab Dulal and Diwa Koirala},
      year={2020},
      eprint={2007.13798},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{koirala-niraula-2021-npvec1,
    title = "{NPV}ec1: Word Embeddings for {N}epali - Construction and Evaluation",
    author = "Koirala, Pravesh  and
      Niraula, Nobal B.",
    editor = "Rogers, Anna  and
      Calixto, Iacer  and
      Vuli{\'c}, Ivan  and
      Saphra, Naomi  and
      Kassner, Nora  and
      Camburu, Oana-Maria  and
      Bansal, Trapit  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.repl4nlp-1.18",
    doi = "10.18653/v1/2021.repl4nlp-1.18",
    pages = "174--184",
    abstract = "Word Embedding maps words to vectors of real numbers. It is derived from a large corpus and is known to capture semantic knowledge from the corpus. Word Embedding is a critical component of many state-of-the-art Deep Learning techniques. However, generating good Word Embeddings is a special challenge for low-resource languages such as Nepali due to the unavailability of large text corpus. In this paper, we present NPVec1 which consists of 25 state-of-art Word Embeddings for Nepali that we have derived from a large corpus using Glove, Word2Vec, FastText, and BERT. We further provide intrinsic and extrinsic evaluations of these Embeddings using well established metrics and methods. These models are trained using 279 million word tokens and are the largest Embeddings ever trained for Nepali language. Furthermore, we have made these Embeddings publicly available to accelerate the development of Natural Language Processing (NLP) applications in Nepali.",
}

@inproceedings{timilsina-etal-2022-nepberta,
    title = "{N}ep{BERT}a: {N}epali Language Model Trained in a Large Corpus",
    author = "Timilsina, Sulav  and
      Gautam, Milan  and
      Bhattarai, Binod",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-short.34",
    pages = "273--284",
    abstract = "Nepali is a low-resource language with more than 40 million speakers worldwide. It is written in Devnagari script and has rich semantics and complex grammatical structure. To this date, multilingual models such as Multilingual BERT, XLM and XLM-RoBERTa haven{'}t been able to achieve promising results in Nepali NLP tasks, and there does not exist any such a large-scale monolingual corpus. This study presents NepBERTa, a BERT-based Natural Language Understanding (NLU) model trained on the most extensive monolingual Nepali corpus ever. We collected a dataset of 0.8B words from 36 different popular news sites in Nepal and introduced the model. This data set is 3 folds times larger than the previous publicly available corpus. We evaluated the performance of NepBERTa in multiple Nepali-specific NLP tasks, including Named-Entity Recognition, Content Classification, POS Tagging, and Sequence Pair Similarity. We also introduce two different datasets for two new downstream tasks and benchmark four diverse NLU tasks altogether. We bring all these four tasks under the first-ever Nepali Language Understanding Evaluation (Nep-gLUE) benchmark. We will make Nep-gLUE along with the pre-trained model and data sets publicly available for research.",
}

@inproceedings{wordembeddinginnepaliword2vec,
author = {Subedi, Bipesh and Poudyal, Prakash},
year = {2023},
month = {06},
pages = {152-156},
title = {Word Embedding in Nepali Language using Word2Vec},
doi = {10.1145/3582768.3582799}
}

@article{Bhatta_Shrestha_Nepal_Pandey_Koirala_2020, title={Efficient Estimation of Nepali Word Representations in Vector Space}, volume={3}, url={https://www.nepjol.info/index.php/jiee/article/view/34327}, DOI={10.3126/jiee.v3i1.34327}, abstractNote={&amp;lt;p&amp;gt;Word representation is a means of representing a word as mathematical entities that can be read, reasoned and manipulated by computational models. The representation is required for input to any new modern data models and in many cases, the accuracy of a model depends on it. In this paper, we analyze various methods of calculating vector space for Nepali words and postulate a word to vector model based on the Skip-gram model with NCE loss capturing syntactic and semantic word relationships.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;This is an attempt to implement a paper by Mikolov on Nepali words.&amp;lt;/p&amp;gt;}, number={1}, journal={Journal of Innovations in Engineering Education}, author={Bhatta, Janardan and Shrestha, Dipesh and Nepal, Santosh and Pandey, Saurav and Koirala, Shekhar}, year={2020}, month={Mar.}, pages={71–77} }

@inproceedings{Singh2019NepaliMT,
  title={Nepali Multi-Class Text Classiﬁcation},
  author={Oyesh Mann Singh},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:220514109}
}

@inproceedings{shahi2018nepali,
  title={Nepali news classification using Naive Bayes, support vector machines and neural networks},
  author={Shahi, Tej Bahadur and Pant, Ashok Kumar},
  booktitle={2018 international conference on communication information and computing technology (iccict)},
  pages={1--5},
  year={2018},
  organization={IEEE}
}

@inproceedings{ghosh2018class,
  title={Class specific TF-IDF boosting for short-text classification: Application to short-texts generated during disasters},
  author={Ghosh, Samujjwal and Desarkar, Maunendra Sankar},
  booktitle={Companion Proceedings of the The Web Conference 2018},
  pages={1629--1637},
  year={2018}
}

@misc{rajan_nepalibert_2021,
  title = {NepaliBERT},
  author = {Rajan},
  year = {2021},
  howpublished = {\url{https://huggingface.co/Rajan/NepaliBERT}}
}

@misc{milanmg_bert-nepali_2022,
  title = {Bert-Nepali},
  author = {Milanmg},
  year = {2022},
  howpublished = {\url{https://huggingface.co/Milanmg/Bert-Nepali}}
}

@data{300D,
doi = {10.21227/dz6s-my90},
url = {https://dx.doi.org/10.21227/dz6s-my90},
author = {Lamsal, Rabindra},
publisher = {IEEE Dataport},
title = {300-Dimensional Word Embeddings for Nepali Language},
year = {2019} } 

@inproceedings{NEURIPS2019_c04c19c2,
 author = {CONNEAU, Alexis and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Cross-lingual Language Model Pretraining},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{wordembedding,
      title={Word Embeddings: A Survey}, 
      author={Felipe Almeida and Geraldo Xexéo},
      year={2023},
      eprint={1901.09069},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{turian-etal-2010-word,
    title = "Word Representations: A Simple and General Method for Semi-Supervised Learning",
    author = "Turian, Joseph  and
      Ratinov, Lev-Arie  and
      Bengio, Yoshua",
    editor = "Haji{\v{c}}, Jan  and
      Carberry, Sandra  and
      Clark, Stephen  and
      Nivre, Joakim",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-1040",
    pages = "384--394",
}


@inproceedings{questionanswer,
author = {Tellex, Stefanie and Katz, Boris and Lin, Jimmy and Fernandes, Aaron and Marton, Gregory},
year = {2003},
month = {07},
pages = {41-47},
title = {Quantitative Evaluation of Passage Retrieval Algorithms for Question Answering},
doi = {10.1145/860435.860445}
}

@inproceedings{sentimentanalysis,
    title = "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions",
    author = "Socher, Richard  and
      Pennington, Jeffrey  and
      Huang, Eric H.  and
      Ng, Andrew Y.  and
      Manning, Christopher D.",
    editor = "Barzilay, Regina  and
      Johnson, Mark",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1014",
    pages = "151--161",
}


@inproceedings {reviewOnWordEmbedding,
        title={ A Comprehensive Review on Word Embedding Techniques },
        authors={ Alahari. Neelima and Shashi Mehrotra },
        
        year={ 2023 },
        
        
        pages={ 538-543 },
        
        
        
        doi={ 10.1109/ICISCoIS56541.2023.10100347 },  
      }

@data{jxrd-d245-20,
doi = {10.21227/jxrd-d245},
url = {https://dx.doi.org/10.21227/jxrd-d245},
author = {Lamsal, Rabindra},
publisher = {IEEE Dataport},
title = {A Large Scale Nepali Text Corpus},
year = {2020} } 

@inproceedings{basnet2018improving,
author = {Basnet, Ashok and Timalsina, Arun},
year = {2018},
month = {10},
pages = {138-142},
title = {Improving Nepali News Recommendation Using Classification Based on LSTM Recurrent Neural Networks},
doi = {10.1109/CCCS.2018.8586815}
}

@inproceedings{kaushal2016,
author = {Kafle, Kaushal and Sharma, Diwas and Subedi, Aayush and Timalsina, Arun},
year = {2018},
month = {03},
pages = {},
title = {Improving Nepali Document Classification by Neural Network}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Gundapu,
author = {Gundapu, Sunil and Mamidi, Radhika},
year = {2021},
month = {01},
pages = {},
title = {Transformer based automatic COVID-19 fake news detection system}
}